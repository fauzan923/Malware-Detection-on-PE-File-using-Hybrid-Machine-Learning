# # INISIALISASI LIBRARY

# # pip install -U scikit-learn
# # from scipy.io import arff
import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns
# import altair as alt
import numpy as np
import re
import joblib
# import ember
# import lightgbm as lgb
# import xgboost
import pathlib

# # from dash import dcc
# from numpy import mean, std
# from sklearn.experimental import enable_hist_gradient_boosting
# from sklearn.ensemble import RandomForestClassifier, VotingClassifier, AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier
# from sklearn.svm import SVC
# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, roc_auc_score
# from sklearn.preprocessing import  StandardScaler, MinMaxScaler
from sklearn.experimental import enable_halving_search_cv 
# from sklearn import model_selection
from sklearn.model_selection import cross_val_score, train_test_split, KFold, GridSearchCV, HalvingGridSearchCV, HalvingRandomSearchCV
from sklearn.feature_selection import VarianceThreshold, RFE, SelectFromModel
# from sklearn.tree import DecisionTreeClassifier
# from sklearn.decomposition import PCA
# from sklearn.pipeline import Pipeline

# from lightgbm import LGBMClassifier
# from mlxtend.classifier import StackingCVClassifier

import streamlit as st
import altair as alt

st.write("""
# Dataset Visualization
## Dataset Distribution
""")

st.sidebar.header('User Input Features')
st.sidebar.markdown("""
[Example CSV input file](https://raw.githubusercontent.com/dataprofessor/data/master/penguins_example.csv)
""")
# Collects user input features into dataframe
uploaded_file = st.sidebar.file_uploader("Upload your input CSV file", type=["csv"])
if uploaded_file is not None:
    input_df = pd.read_csv(uploaded_file)
else:
    # INISIALISASI VARIABEL X DAN Y
    df_columns = pd.read_csv('Bodmas/ember_2381_feature_names.txt', delimiter='\n')
    df_columns = list(df_columns['0'])

    # DATA PREPROCESSING
    filename = 'Bodmas/bodmas.npz'
    data = np.load(filename)
    X = data['X']  # all the feature vectors
    y = data['y']  # labels, 0 as benign, 1 as malicious


    # INISIALISASI VARIABEL X DAN Y
    X = pd.DataFrame(X, columns=df_columns)
    y = pd.DataFrame(y)

    # INISIALISASI VARIABEL DATAFRAME DF
    df = pd.DataFrame(X)
    df['Label'] = pd.DataFrame(y)
    df = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))

    # MEMILIH FEATURE YANG MEMILIKI NILAI VARIANCE NYA 0
    # It will drop low variance features towards the dataset
    thresh = VarianceThreshold(threshold=0)
    thresh.fit(df)
    thresh.get_support()

    # columns that have 0 threshold
    columns_not_used = [column for column in df.columns if column not in df.columns[thresh.get_support()]]

    # NGE DROP FEATURE YANG MEMILIKI NILAI VARIANCE NYA 0
    # Feature Selection Using Correlation Between Feature
    df = df.drop(columns_not_used,axis=1)

    # DROP DATA YANG KOSONG
    df = df.dropna()
    # df.shape
    X = df.drop(['Label'], axis=1)
    y = df['Label']

    # TRAIN TEST SPLIT DATA
    X_train, X_test, y_train, y_test = train_test_split(X,y,stratify=y,random_state=0,test_size=0.2)

    # Visualisasi dataset distribution
    group_label = df.groupby(["Label"]).count().reset_index()
    dist_bar_chart = alt.Chart(group_label).mark_bar().encode(
        alt.X('Label:O', axis=alt.Axis(title='Label')),
        alt.Y('ByteHistorgram0:Q', axis=alt.Axis(title='Number of data')),
        alt.Color('Label:N', scale=alt.Scale(range=["#3333ff", "#ff3333"]), legend=alt.Legend(values=["benign", "malicious"]))
    ).properties(
        width=400,
        height=400
    )
    st.altair_chart(dist_bar_chart)

    # Visualisasi Relasi antar Dataset
    st.write('## Visualisasi Relasi antar Data Sample')
    X_Axis = st.sidebar.selectbox('X-Axis',(df.columns))
    df2 = df.drop(X_Axis, axis=1)
    Y_Axis = st.sidebar.selectbox('Y-Axis',(df2.columns))
    data_sample = st.sidebar.slider('Berapa banyak sample yang mau dilihat?',min_value=50,max_value=10000)
    relation_scatter_chart = alt.Chart(df.sample(n=data_sample, axis=0)).mark_circle().encode(
        x='{}:Q'.format(X_Axis),
        y='{}:Q'.format(Y_Axis)
    ).properties(
        width=800,
        height=800
    )
    st.altair_chart(relation_scatter_chart.transform_sample(100))

    # NGEDROP FEATURE YANG TIDAK TERPAKAI PADA df_PE SESUAI DENGAN HASIL SELEKSI FITUR YANG SUDAH PERNAH DILAKUKAN
    selected_feat = pd.read_csv('output.txt', delimiter='\n')
    selected_feat = list(selected_feat['0'])
    columns_train_not_used = [column for column in df.columns if column not in selected_feat]
    df = df.drop(columns_train_not_used,axis=1)

    # Ngeload hasil dari model hybrid machine learning yang sudah di training 
    sclfjl = joblib.load('sclf_joblib')
    sclfjl_pred = sclfjl.predict(df)
    sclf_pred_prob = sclfjl.predict_proba(df)


































# st.sidebar.header('User Input Features')
# st.sidebar.markdown("""
# [Example CSV input file](https://raw.githubusercontent.com/dataprofessor/data/master/penguins_example.csv)
# """)
# # Collects user input features into dataframe
# uploaded_file = st.sidebar.file_uploader("Upload your input CSV file", type=["csv"])
# if uploaded_file is not None:
#     input_df = pd.read_csv(uploaded_file)
# else:
#     def user_input_features():
#         island = st.sidebar.selectbox('Island',('Biscoe','Dream','Torgersen'))
#         sex = st.sidebar.selectbox('Sex',('male','female'))
#         bill_length_mm = st.sidebar.slider('Bill length (mm)', 32.1,59.6,43.9)
#         bill_depth_mm = st.sidebar.slider('Bill depth (mm)', 13.1,21.5,17.2)
#         flipper_length_mm = st.sidebar.slider('Flipper length (mm)', 172.0,231.0,201.0)
#         body_mass_g = st.sidebar.slider('Body mass (g)', 2700.0,6300.0,4207.0)
#         data = {'island': island,
#                 'bill_length_mm': bill_length_mm,
#                 'bill_depth_mm': bill_depth_mm,
#                 'flipper_length_mm': flipper_length_mm,
#                 'body_mass_g': body_mass_g,
#                 'sex': sex}
#         features = pd.DataFrame(data, index=[0])
#         return features
#     input_df = user_input_features()

# # X_Axis = st.sidebar.selectbox('X-Axis',(df.columns))
# # df2 = df.drop(X_Axis, axis=1)
# # Y_Axis = st.sidebar.selectbox('Y-Axis',(df2.columns))






# # https://towardsdatascience.com/how-to-get-stock-data-using-python-c0de1df17e75
# #define the ticker symbol
# tickerSymbol = 'GOOGL'
# #get data on this ticker
# tickerData = yf.Ticker(tickerSymbol)
# #get the historical prices for this ticker
# tickerDf = tickerData.history(period='1d', start='2010-5-31', end='2020-5-31')
# # Open	High	Low	Close	Volume	Dividends	Stock Splits

# st.write("""
# ## Closing Price
# """)
# st.line_chart(tickerDf.Close)
# st.write("""
# ## Volume Price
# """)
# st.line_chart(tickerDf.Volume)

# st.write("""
# # Penguin Prediction App

# This app predicts the **Palmer Penguin** species!

# Data obtained from the [palmerpenguins library](https://github.com/allisonhorst/palmerpenguins) in R by Allison Horst.
# """)



# # Combines user input features with entire penguins dataset
# # This will be useful for the encoding phase
# penguins_raw = pd.read_csv('penguins_cleaned.csv')
# penguins = penguins_raw.drop(columns=['species'])
# df = pd.concat([input_df,penguins],axis=0)

# # Encoding of ordinal features
# # https://www.kaggle.com/pratik1120/penguin-dataset-eda-classification-and-clustering
# encode = ['sex','island']
# for col in encode:
#     dummy = pd.get_dummies(df[col], prefix=col)
#     df = pd.concat([df,dummy], axis=1)
#     del df[col]
# df = df[:1] # Selects only the first row (the user input data)

# # Displays the user input features
# st.subheader('User Input features')

# if uploaded_file is not None:
#     st.write(df)
# else:
#     st.write('Awaiting CSV file to be uploaded. Currently using example input parameters (shown below).')
#     st.write(df)